# -*- coding: utf-8 -*-
"""assignment8_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iQeHjMOUN_jK7oMwlTGzRbQ8tzXbLjA1
"""

!pip install pandas faiss-cpu sentence-transformers transformers openai

!pip install transformers

from google.colab import files
import pandas as pd

# Upload the CSV file
uploaded = files.upload()

# Load the dataset
filename = next(iter(uploaded))  # gets uploaded filename
df = pd.read_csv(filename)

# Preview the data
df.head()

def row_to_text(row):
    return (
        f"Loan ID {row['Loan_ID']}: A {row['Gender']} "
        f"{'Married' if row['Married'] == 'Yes' else 'Unmarried'}, "
        f"{row['Education']} applicant with income {row['ApplicantIncome']} "
        f"and co-applicant income {row['CoapplicantIncome']}, "
        f"loan amount {row['LoanAmount']} over term {row['Loan_Amount_Term']}, "
        f"credit history {row['Credit_History']}, "
        f"property area {row['Property_Area']}. "
        f"Loan was {'approved' if row['Loan_Status'] == 'Y' else 'rejected'}."
    )

# Fill missing values first to avoid errors
df = df.fillna(value={col: "Unknown" for col in df.columns})
df = df.astype(str)  # convert all columns to string to avoid type mismatch
# Create a new column with natural language context
df['context'] = df.apply(row_to_text, axis=1)

# Store all contexts in a list
contexts = df['context'].tolist()

# Show a few examples
for i in range(3):
    print(f"Context {i+1}:\n{contexts[i]}\n")

from sentence_transformers import SentenceTransformer

# Rebuild contexts list from the latest DataFrame
contexts = df['context'].tolist()

# Reload the model
model = SentenceTransformer('all-MiniLM-L6-v2')
contexts = contexts[:100]  # Optional: for testing only
# Rebuild embeddings to match the updated contexts
embeddings = model.encode(
    contexts,
    show_progress_bar=True,
    convert_to_numpy=True,
    normalize_embeddings=True
)

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def retrieve_similar_contexts(query, top_k=3):
    query_embedding = model.encode([query])
    similarities = cosine_similarity(query_embedding, embeddings)[0]
    top_indices = similarities.argsort()[-top_k:][::-1]

    # Ensure no index is out of range
    top_indices = [i for i in top_indices if i < len(contexts)]

    return [contexts[i] for i in top_indices]

from transformers import pipeline
import torch

# Load the QA model (use a small instruction-tuned model)
qa_pipeline = pipeline(
    "text-generation",
    model="tiiuae/falcon-7b-instruct",
    device=0 if torch.cuda.is_available() else -1
)

def generate_answer_hf(query, retrieved_contexts):
    context = "\n".join(retrieved_contexts)
    prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
    result = qa_pipeline(prompt, max_length=200, do_sample=True)[0]["generated_text"]
    return result.split("Answer:")[-1].strip()

import openai

openai_api_key = "sk..."  # â›” Replace with your real API key (no quotes or extra spaces)
client = openai.OpenAI(api_key=openai_api_key)

def generate_answer_openai(query, retrieved_contexts):
    context = "\n".join(retrieved_contexts)
    prompt = (
        f"Based on the information below, answer the question.\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {query}\n"
        f"Answer:"
    )

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # or gpt-4
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=200,
    )

    return response.choices[0].message.content.strip()

query = "What was the loan status of the applicant with income 3000?"
contexts = retrieve_similar_contexts(query)
answer = generate_answer_hf(query, contexts)

print("Answer:", answer)

!pip install -q transformers accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from huggingface_hub import login
login("your login")

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    torch_dtype=torch.float16,
    device_map="auto"
)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    use_auth_token=True
)

def chat_with_model(prompt, max_new_tokens=256):
    # Format prompt (modify as per your model's instruction format)
    system_prompt = f"<s>[INST] {prompt} [/INST]"

    inputs = tokenizer(system_prompt, return_tensors="pt").to(model.device)

    # Generate response
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.2
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True)

    # Clean and return only model's answer
    answer = response.split("[/INST]")[-1].strip()
    return answer

print(chat_with_model("Explain what is a transformer in deep learning."))